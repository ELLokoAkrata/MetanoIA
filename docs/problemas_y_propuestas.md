# Problemas y Propuestas

## Problemas Identificados

### 2025-05-09: Error de importaciÃ³n en chat_bot.py
**Problema**: Se detectÃ³ un error en el archivo `chat_bot.py` donde se intentaba utilizar el mÃ³dulo `os` sin haberlo importado previamente, lo que generaba el siguiente error:
```
NameError: name 'os' is not defined
```

**Impacto**: La aplicaciÃ³n no podÃ­a ejecutarse correctamente ya que no podÃ­a acceder a la clave API de Groq almacenada en las variables de entorno.

### 2025-05-09: Funcionalidad bÃ¡sica limitada
**Problema**: La aplicaciÃ³n solo tenÃ­a una configuraciÃ³n bÃ¡sica sin funcionalidad de chat implementada y sin soporte para mÃºltiples modelos.

**Impacto**: No era posible interactuar con el chatbot ni configurar diferentes parÃ¡metros para la generaciÃ³n de respuestas.

### 2025-05-09: Problemas de legibilidad en la interfaz
**Problema**: La interfaz tenÃ­a problemas de contraste, con contenedores blancos que dificultaban la lectura del texto.

**Impacto**: La experiencia de usuario se veÃ­a afectada por la dificultad para leer el contenido de la aplicaciÃ³n.

### 2025-05-09: Cambio de modelo no efectivo
**Problema**: Al cambiar de modelo en la barra lateral, no se aplicaba correctamente el cambio y se seguÃ­a utilizando el modelo predeterminado.

**Impacto**: No era posible probar y comparar diferentes modelos de lenguaje, limitando la funcionalidad principal de la aplicaciÃ³n.

### 2025-05-09: Error al enviar campos personalizados a la API
**Problema**: Se estaban enviando campos personalizados (`model_used`) en los mensajes a la API de Groq, lo que generaba un error:
```
Error code: 400 - {'error': {'message': "'messages.2' : for 'role:assistant' the following must be satisfied[('messages.2' : property 'model_used' is unsupported, did you mean 'role'?)]"}
```

**Impacto**: La aplicaciÃ³n fallaba al intentar generar respuestas despuÃ©s de cambiar de modelo.

### 2025-05-09: LÃ­mite de tokens excedido en modelos de Llama
**Problema**: Al acumular muchos mensajes en una conversaciÃ³n, se excede el lÃ­mite de tokens por minuto (TPM) de la API de Groq, especialmente con los modelos Llama:
```
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `meta-llama/llama-4-maverick-17b-128e-instruct` in organization `org_01hvyf1saverv8m45x6879pnc4` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6029, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
```

**Impacto**: La aplicaciÃ³n no podÃ­a generar respuestas cuando la conversaciÃ³n se volvÃ­a demasiado larga, limitando la utilidad del chatbot en conversaciones extensas.

## Propuestas de SoluciÃ³n

### 2025-05-09: Agregar importaciÃ³n faltante
**Propuesta**: Agregar la importaciÃ³n del mÃ³dulo `os` al principio del archivo.

**ImplementaciÃ³n**: Se modificÃ³ el archivo `chat_bot.py` para incluir la importaciÃ³n:
```python
import os
import streamlit as st
from groq import Groq
```

**Estado**: âœ… Implementado

### 2025-05-09: Implementar interfaz de chat completa
**Propuesta**: Desarrollar una interfaz de chat completa con soporte para mÃºltiples modelos, configuraciÃ³n de parÃ¡metros y streaming de respuestas.

**ImplementaciÃ³n**: Se reescribiÃ³ completamente el archivo `chat_bot.py` para incluir funcionalidades avanzadas de chat.

**Estado**: âœ… Implementado

### 2025-05-09: Implementar tema "Fresh Tech"
**Propuesta**: Mejorar la interfaz con un diseÃ±o moderno y tecnolÃ³gico que resuelva los problemas de legibilidad.

**ImplementaciÃ³n**: Se creÃ³ un tema personalizado con gradientes, efectos de vidrio y mejor contraste:
```css
/* Tema base con gradiente para toda la aplicaciÃ³n */
.stApp {
    background: linear-gradient(135deg, #0f172a 0%, #1e293b 100%) !important;
    color: #e2e8f0 !important;
}

/* Estilos para mensajes de chat con efecto de vidrio */
.stChatMessage {
    background-color: rgba(30, 41, 59, 0.7) !important;
    backdrop-filter: blur(10px) !important;
    border-radius: 12px !important;
    /* MÃ¡s estilos... */
}
```

**Estado**: âœ… Implementado

### 2025-05-09: Implementar sistema de registro y corregir cambio de modelo
**Propuesta**: AÃ±adir un sistema de logging detallado y corregir el problema con el cambio de modelos.

**ImplementaciÃ³n**: 
1. Se agregÃ³ un sistema de logging completo:
```python
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger("psycho-bot")
```

2. Se modificÃ³ la selecciÃ³n de modelo para forzar una recarga cuando cambia:
```python
if (selected_model != st.session_state.context["model"]):
    logger.info(f"Cambio de modelo: {st.session_state.context['model']} -> {selected_model}")
    st.session_state.context["model"] = selected_model
    # Forzar recarga para aplicar el cambio inmediatamente
    st.rerun()
```

3. Se implementÃ³ un seguimiento del modelo usado para cada respuesta:
```python
st.session_state.messages.append({"role": "assistant", "content": full_response, "model_used": current_model})
```

**Estado**: âœ… Implementado

### 2025-05-09: Corregir error de campos personalizados en la API
**Propuesta**: Filtrar los campos personalizados antes de enviar los mensajes a la API de Groq.

**ImplementaciÃ³n**: Se modificÃ³ la preparaciÃ³n de mensajes para la API:
```python
# Preparar mensajes para la API (filtrando campos personalizados)
api_messages = [
    {"role": "system", "content": st.session_state.context["system_prompt"]}
]

# AÃ±adir mensajes del historial filtrando campos personalizados
for msg in st.session_state.messages:
    if msg["role"] in ["user", "assistant"]:
        # Solo incluir campos estÃ¡ndar (role y content)
        api_messages.append({"role": msg["role"], "content": msg["content"]})
```

**Estado**: âœ… Implementado

### 2025-05-09: Implementar limitaciÃ³n dinÃ¡mica del contexto
**Propuesta**: Limitar dinÃ¡micamente el nÃºmero de mensajes enviados a la API segÃºn el modelo utilizado para evitar exceder los lÃ­mites de tokens por minuto (TPM).

**ImplementaciÃ³n**: Se modificÃ³ el cÃ³digo para limitar el contexto de forma dinÃ¡mica segÃºn el modelo:
```python
# Obtener el nÃºmero mÃ¡ximo de mensajes a incluir segÃºn el modelo
max_context_messages = 10  # Valor predeterminado

# Ajustar el contexto segÃºn el modelo para evitar errores de lÃ­mite de tokens
if "llama-4-maverick" in current_model:
    max_context_messages = 5  # Limitar mÃ¡s para modelos que tienen lÃ­mites mÃ¡s estrictos
elif "llama-4-scout" in current_model:
    max_context_messages = 6

# Registrar el lÃ­mite de contexto aplicado
logger.info(f"Limitando contexto a {max_context_messages} mensajes para el modelo {current_model}")

# AÃ±adir mensajes del historial filtrando campos personalizados y limitando la cantidad
# Tomamos solo los mensajes mÃ¡s recientes para no exceder los lÃ­mites
recent_messages = st.session_state.messages[-max_context_messages:] if len(st.session_state.messages) > max_context_messages else st.session_state.messages
```

**Estado**: âœ… Implementado

## Propuestas Futuras

### Manejo seguro de claves API
**Propuesta**: Implementar un mÃ©todo mÃ¡s seguro para manejar las claves API, como utilizar un archivo `.env` con la biblioteca `python-dotenv`.

**Beneficios**:
- Mayor seguridad para las credenciales
- Facilidad para configurar el entorno de desarrollo
- PrevenciÃ³n de exposiciÃ³n accidental de claves en el cÃ³digo fuente

**Estado**: ğŸ“ Pendiente de implementaciÃ³n

### Guardado de conversaciones
**Propuesta**: Implementar funcionalidad para guardar y cargar conversaciones anteriores.

**Beneficios**:
- PermitirÃ­a a los usuarios continuar conversaciones anteriores
- FacilitarÃ­a el anÃ¡lisis de conversaciones para mejorar el sistema
- MejorarÃ­a la experiencia de usuario al mantener un historial accesible

**Estado**: ğŸ“ Pendiente de implementaciÃ³n

### Procesamiento de archivos
**Propuesta**: AÃ±adir soporte para cargar y procesar archivos (PDF, texto, etc.) como contexto para las conversaciones.

**Beneficios**:
- PermitirÃ­a al chatbot responder preguntas basadas en documentos especÃ­ficos
- AmpliarÃ­a las capacidades del sistema para casos de uso mÃ¡s avanzados
- FacilitarÃ­a la creaciÃ³n de asistentes especializados en dominios especÃ­ficos

**Estado**: ğŸ“ Pendiente de implementaciÃ³n

### ComparaciÃ³n de modelos lado a lado
**Propuesta**: Implementar una funcionalidad para comparar respuestas de diferentes modelos a la misma pregunta.

**Beneficios**:
- FacilitarÃ­a la evaluaciÃ³n de la calidad de diferentes modelos
- PermitirÃ­a identificar las fortalezas y debilidades de cada modelo
- MejorarÃ­a la experiencia educativa al mostrar diferentes enfoques

**Estado**: ğŸ“ Pendiente de implementaciÃ³n

### AnÃ¡lisis de rendimiento y uso de tokens
**Propuesta**: AÃ±adir mÃ©tricas de rendimiento y conteo de tokens para cada modelo.

**Beneficios**:
- PermitirÃ­a optimizar el uso de recursos
- FacilitarÃ­a la comparaciÃ³n de eficiencia entre modelos
- AyudarÃ­a a estimar costos de uso de la API

**Estado**: ğŸ“ Pendiente de implementaciÃ³n
