# Problemas y Propuestas

## Problemas Identificados

### 2025-05-10: Inconsistencia en el cambio de modelo
**Problema**: Al cambiar de modelo en la interfaz, en ocasiones el cambio no se aplica realmente y se sigue utilizando el modelo anterior, a pesar de que la interfaz muestra el nuevo modelo seleccionado.

**Impacto**: Genera confusi√≥n en los usuarios que creen estar utilizando un modelo espec√≠fico cuando en realidad est√°n usando otro. Esto afecta la experiencia de usuario y la confiabilidad de la aplicaci√≥n, especialmente cuando se est√°n comparando diferentes modelos.

### 2025-05-09: Error de importaci√≥n en chat_bot.py
**Problema**: Se detect√≥ un error en el archivo `chat_bot.py` donde se intentaba utilizar el m√≥dulo `os` sin haberlo importado previamente, lo que generaba el siguiente error:
```
NameError: name 'os' is not defined
```

**Impacto**: La aplicaci√≥n no pod√≠a ejecutarse correctamente ya que no pod√≠a acceder a la clave API de Groq almacenada en las variables de entorno.

### 2025-05-09: Funcionalidad b√°sica limitada
**Problema**: La aplicaci√≥n solo ten√≠a una configuraci√≥n b√°sica sin funcionalidad de chat implementada y sin soporte para m√∫ltiples modelos.

**Impacto**: No era posible interactuar con el chatbot ni configurar diferentes par√°metros para la generaci√≥n de respuestas.

### 2025-05-09: Problemas de legibilidad en la interfaz
**Problema**: La interfaz ten√≠a problemas de contraste, con contenedores blancos que dificultaban la lectura del texto.

**Impacto**: La experiencia de usuario se ve√≠a afectada por la dificultad para leer el contenido de la aplicaci√≥n.

### 2025-05-09: Cambio de modelo no efectivo
**Problema**: Al cambiar de modelo en la barra lateral, no se aplicaba correctamente el cambio y se segu√≠a utilizando el modelo predeterminado.

**Impacto**: No era posible probar y comparar diferentes modelos de lenguaje, limitando la funcionalidad principal de la aplicaci√≥n.

### 2025-05-09: Error al enviar campos personalizados a la API
**Problema**: Se estaban enviando campos personalizados (`model_used`) en los mensajes a la API de Groq, lo que generaba un error:
```
Error code: 400 - {'error': {'message': "'messages.2' : for 'role:assistant' the following must be satisfied[('messages.2' : property 'model_used' is unsupported, did you mean 'role'?)]"}
```

**Impacto**: La aplicaci√≥n fallaba al intentar generar respuestas despu√©s de cambiar de modelo.

### 2025-05-09: L√≠mite de tokens excedido en modelos de Llama
**Problema**: Al acumular muchos mensajes en una conversaci√≥n, se excede el l√≠mite de tokens por minuto (TPM) de la API de Groq, especialmente con los modelos Llama:
```
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `meta-llama/llama-4-maverick-17b-128e-instruct` in organization `org_01hvyf1saverv8m45x6879pnc4` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6029, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
```

**Impacto**: La aplicaci√≥n no pod√≠a generar respuestas cuando la conversaci√≥n se volv√≠a demasiado larga, limitando la utilidad del chatbot en conversaciones extensas.

### 2025-05-09: Error de importaci√≥n en la integraci√≥n de herramientas ag√©nticas
**Problema**: Al implementar la integraci√≥n de herramientas ag√©nticas, se produjo un error por falta de importaci√≥n de la funci√≥n `display_agentic_context` en el archivo principal:
```
2025-05-09 16:29:22.251 Uncaught app execution
Traceback (most recent call last):
  File "C:\Python311\Lib\site-packages\streamlit\runtime\scriptrunner\exec_code.py", line 88, in exec_func_with_error_handling
    result = func()
             ^^^^^^
  File "C:\Python311\Lib\site-packages\streamlit\runtime\scriptrunner\script_runner.py", line 579, in code_to_exec
    exec(code, module.__dict__)
  File "C:\Users\Ricardo Ruiz\Desktop\MetanoIA\app.py", line 67, in <module>
    main()
  File "C:\Users\Ricardo Ruiz\Desktop\MetanoIA\app.py", line 60, in main
    display_agentic_context(session_state)
    ^^^^^^^^^^^^^^^^^^^^^^^
NameError: name 'display_agentic_context' is not defined
```

**Impacto**: La aplicaci√≥n no pod√≠a iniciarse despu√©s de implementar las herramientas ag√©nticas, impidiendo probar la nueva funcionalidad.

### 2025-05-09: Error en el procesamiento de herramientas ag√©nticas
**Problema**: Al procesar las herramientas ag√©nticas ejecutadas, se produjo un error cuando el campo "output" era un string en lugar de un diccionario:
```
Traceback (most recent call last):
  File "C:\Python311\Lib\site-packages\streamlit\runtime\scriptrunner\exec_code.py", line 88, in exec_func_with_error_handling
    result = func()
             ^^^^^^
  File "C:\Python311\Lib\site-packages\streamlit\runtime\scriptrunner\script_runner.py", line 579, in code_to_exec
    exec(code, module.__dict__)
  File "C:\Users\Ricardo Ruiz\Desktop\MetanoIA\app.py", line 67, in <module>
    main()
  File "C:\Users\Ricardo Ruiz\Desktop\MetanoIA\app.py", line 64, in main
    handle_user_input(prompt, session_state, groq_client, logger)
  File "C:\Users\Ricardo Ruiz\Desktop\MetanoIA\src\components\chat.py", line 195, in handle_user_input
    agentic_tools_manager.process_executed_tools(executed_tools)
  File "C:\Users\Ricardo Ruiz\Desktop\MetanoIA\src\utils\agentic_tools_manager.py", line 99, in process_executed_tools
    "results": tool.get("output", {}).get("results", []),
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
```

**Impacto**: La aplicaci√≥n fallaba al intentar procesar las herramientas ag√©nticas cuando el formato de respuesta de la API no era el esperado, impidiendo el uso efectivo de las capacidades de b√∫squeda web y ejecuci√≥n de c√≥digo.

## Propuestas de Soluci√≥n

### 2025-05-09: Agregar importaci√≥n faltante
**Propuesta**: Agregar la importaci√≥n del m√≥dulo `os` al principio del archivo.

**Implementaci√≥n**: Se modific√≥ el archivo `chat_bot.py` para incluir la importaci√≥n:
```python
import os
import streamlit as st
from groq import Groq
```

**Estado**: ‚úÖ Implementado

### 2025-05-09: Implementar interfaz de chat completa
**Propuesta**: Desarrollar una interfaz de chat completa con soporte para m√∫ltiples modelos, configuraci√≥n de par√°metros y streaming de respuestas.

**Implementaci√≥n**: Se reescribi√≥ completamente el archivo `chat_bot.py` para incluir funcionalidades avanzadas de chat.

**Estado**: ‚úÖ Implementado

### 2025-05-09: Implementar tema "Fresh Tech"
**Propuesta**: Mejorar la interfaz con un dise√±o moderno y tecnol√≥gico que resuelva los problemas de legibilidad.

**Implementaci√≥n**: Se cre√≥ un tema personalizado con gradientes, efectos de vidrio y mejor contraste:
```css
/* Tema base con gradiente para toda la aplicaci√≥n */
.stApp {
    background: linear-gradient(135deg, #0f172a 0%, #1e293b 100%) !important;
    color: #e2e8f0 !important;
}

/* Estilos para mensajes de chat con efecto de vidrio */
.stChatMessage {
    background-color: rgba(30, 41, 59, 0.7) !important;
    backdrop-filter: blur(10px) !important;
    border-radius: 12px !important;
    /* M√°s estilos... */
}
```

**Estado**: ‚úÖ Implementado

### 2025-05-09: Implementar sistema de registro y corregir cambio de modelo
**Propuesta**: A√±adir un sistema de logging detallado y corregir el problema con el cambio de modelos.

**Implementaci√≥n**: 
1. Se agreg√≥ un sistema de logging completo:
```python
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger("psycho-bot")
```

2. Se modific√≥ la selecci√≥n de modelo para forzar una recarga cuando cambia:
```python
if (selected_model != st.session_state.context["model"]):
    logger.info(f"Cambio de modelo: {st.session_state.context['model']} -> {selected_model}")
    st.session_state.context["model"] = selected_model
    # Forzar recarga para aplicar el cambio inmediatamente
    st.rerun()
```

3. Se implement√≥ un seguimiento del modelo usado para cada respuesta:
```python
st.session_state.messages.append({"role": "assistant", "content": full_response, "model_used": current_model})
```

**Estado**: ‚úÖ Implementado

### 2025-05-09: Corregir error de campos personalizados en la API
**Propuesta**: Filtrar los campos personalizados antes de enviar los mensajes a la API de Groq.

**Implementaci√≥n**: Se modific√≥ la preparaci√≥n de mensajes para la API:
```python
# Preparar mensajes para la API (filtrando campos personalizados)
api_messages = [
    {"role": "system", "content": st.session_state.context["system_prompt"]}
]

# A√±adir mensajes del historial filtrando campos personalizados
for msg in st.session_state.messages:
    if msg["role"] in ["user", "assistant"]:
        # Solo incluir campos est√°ndar (role y content)
        api_messages.append({"role": msg["role"], "content": msg["content"]})
```

**Estado**: ‚úÖ Implementado

### 2025-05-09: Implementar limitaci√≥n din√°mica del contexto
**Propuesta**: Limitar din√°micamente el n√∫mero de mensajes enviados a la API seg√∫n el modelo utilizado para evitar exceder los l√≠mites de tokens por minuto (TPM).

**Implementaci√≥n**: Se modific√≥ el c√≥digo para limitar el contexto de forma din√°mica seg√∫n el modelo:
```python
# Obtener el n√∫mero m√°ximo de mensajes a incluir seg√∫n el modelo
max_context_messages = 10  # Valor predeterminado

# Ajustar el contexto seg√∫n el modelo para evitar errores de l√≠mite de tokens
if "llama-4-maverick" in current_model:
    max_context_messages = 5  # Limitar m√°s para modelos que tienen l√≠mites m√°s estrictos
elif "llama-4-scout" in current_model:
    max_context_messages = 6

# Registrar el l√≠mite de contexto aplicado
logger.info(f"Limitando contexto a {max_context_messages} mensajes para el modelo {current_model}")

# A√±adir mensajes del historial filtrando campos personalizados y limitando la cantidad
# Tomamos solo los mensajes m√°s recientes para no exceder los l√≠mites
recent_messages = st.session_state.messages[-max_context_messages:] if len(st.session_state.messages) > max_context_messages else st.session_state.messages
```

**Estado**: ‚úÖ Implementado

### 2025-05-09: Corregir importaci√≥n faltante en app.py
**Propuesta**: A√±adir la importaci√≥n de la funci√≥n `display_agentic_context` desde el m√≥dulo de chat al archivo principal.

**Implementaci√≥n**: Se modific√≥ la secci√≥n de importaciones en el archivo `app.py`:
```python
# Importar m√≥dulos propios
from src.utils.logger import setup_logger
from src.utils.styles import apply_fresh_tech_theme
from src.utils.session_state import initialize_session_state
from src.models.config import AVAILABLE_MODELS
from src.api.groq_client import GroqClient
from src.components.sidebar import render_sidebar
from src.components.chat import display_chat_history, handle_user_input, display_agentic_context
```

**Estado**: ‚úÖ Implementado

### 2025-05-09: Mejorar la robustez del procesamiento de herramientas ag√©nticas
**Propuesta**: Modificar el m√©todo `process_executed_tools` en la clase `AgenticToolsManager` para manejar correctamente diferentes formatos de respuesta de la API, incluyendo cuando los campos "input" u "output" son strings en lugar de diccionarios.

**Implementaci√≥n**: Se modific√≥ el m√©todo para incluir verificaci√≥n de tipos y manejo de excepciones:
```python
def process_executed_tools(self, executed_tools):
    """
    Procesa las herramientas ejecutadas y las a√±ade al contexto.
    
    Args:
        executed_tools (list): Lista de herramientas ejecutadas.
    """
    if not executed_tools:
        return
    
    for tool in executed_tools:
        try:
            # Verificar que tool sea un diccionario
            if not isinstance(tool, dict):
                logger.warning(f"Herramienta no es un diccionario: {tool}")
                continue
            
            tool_type = tool.get("type")
            
            # Obtener input y output de forma segura
            tool_input = tool.get("input", {})
            tool_output = tool.get("output", {})
            
            # Convertir a diccionario si son strings
            if isinstance(tool_input, str):
                logger.warning(f"Input es un string: {tool_input}")
                tool_input = {"raw": tool_input}
            
            if isinstance(tool_output, str):
                logger.warning(f"Output es un string: {tool_output}")
                tool_output = {"raw": tool_output}
            
            # Procesar seg√∫n el tipo de herramienta
            # Resto del c√≥digo...
        except Exception as e:
            logger.error(f"Error al procesar herramienta: {str(e)}")
            logger.exception("Detalles del error:")
            continue
```

**Estado**: ‚úÖ Implementado

## Propuestas de Soluci√≥n

### 2025-05-10: Corregir inconsistencia en el cambio de modelo
**Propuesta**: Implementar un mecanismo m√°s robusto para el cambio de modelo que garantice que el modelo seleccionado se aplique correctamente, incluyendo una verificaci√≥n expl√≠cita y un reinicio de la sesi√≥n cuando sea necesario.

**Implementaci√≥n**: 
Despu√©s de analizar el c√≥digo, se identificaron dos problemas principales:

1. **Problema de actualizaci√≥n del estado**: Aunque el cambio de modelo se registra en `session_state.context["model"]`, no se fuerza un reinicio completo de la aplicaci√≥n, lo que puede causar que el cambio no se aplique correctamente.

2. **Problema con el widget de selecci√≥n**: El widget de selecci√≥n de modelo no siempre refleja correctamente el modelo actual debido a c√≥mo Streamlit maneja el estado de los widgets.

Se implementaron las siguientes soluciones:

1. **Uso de key din√°mica para el selectbox**: Se modific√≥ el c√≥digo para usar una key din√°mica basada en el modelo actual, forzando a Streamlit a recrear el widget cuando cambia el modelo:

```python
# En src/components/sidebar.py
selected_model = st.selectbox(
    "Selecciona un modelo",
    options=list(AVAILABLE_MODELS.keys()),
    format_func=lambda x: AVAILABLE_MODELS[x],
    index=list(AVAILABLE_MODELS.keys()).index(session_state.context["model"]) 
        if session_state.context["model"] in AVAILABLE_MODELS else 0,
    key=f"model_select_{session_state.context['model']}"  # Key din√°mica basada en el modelo actual
)
```

2. **Forzar reinicio de la aplicaci√≥n**: Se modific√≥ la funci√≥n `render_sidebar` para devolver `True` cuando cambia el modelo, lo que indica a la aplicaci√≥n principal que debe reiniciarse:

```python
# En src/components/sidebar.py
if (selected_model != session_state.context["model"]):
    logger.info(f"Cambio de modelo: {session_state.context['model']} -> {selected_model}")
    session_state.context["model"] = selected_model
    config_changed = True  # Esto har√° que la aplicaci√≥n se reinicie
```

3. **Verificaci√≥n expl√≠cita del modelo antes de cada generaci√≥n**: Se a√±adi√≥ c√≥digo en `handle_user_input` para verificar que el modelo seleccionado sea el que realmente se est√° utilizando:

```python
# En src/components/chat.py
def handle_user_input(prompt, session_state, groq_client, logger):
    # Verificar y registrar el modelo actual
    current_model = session_state.context["model"]
    model_obj = get_model(current_model)
    logger.info(f"Usando modelo: {current_model} ({get_model_display_name(current_model)})")
    
    # Resto del c√≥digo...
```

4. **Indicador visual del modelo en uso**: Se a√±adi√≥ un indicador claro en la interfaz que muestra qu√© modelo se est√° utilizando actualmente:

```python
# En app.py
st.markdown(f"**Modelo actual:** {get_model_display_name(session_state.context['model'])}")
```

**Estado**: ‚úÖ Implementado

## Propuestas Futuras

### Manejo seguro de claves API
**Propuesta**: Implementar un m√©todo m√°s seguro para manejar las claves API, como utilizar un archivo `.env` con la biblioteca `python-dotenv`.

**Beneficios**:
- Mayor seguridad para las credenciales
- Facilidad para configurar el entorno de desarrollo
- Prevenci√≥n de exposici√≥n accidental de claves en el c√≥digo fuente

**Estado**: üìù Pendiente de implementaci√≥n

### Guardado de conversaciones
**Propuesta**: Implementar funcionalidad para guardar y cargar conversaciones anteriores.

**Beneficios**:
- Permitir√≠a a los usuarios continuar conversaciones anteriores
- Facilitar√≠a el an√°lisis de conversaciones para mejorar el sistema
- Mejorar√≠a la experiencia de usuario al mantener un historial accesible

**Estado**: üìù Pendiente de implementaci√≥n

### Procesamiento de archivos
**Propuesta**: A√±adir soporte para cargar y procesar archivos (PDF, texto, etc.) como contexto para las conversaciones.

**Beneficios**:
- Permitir√≠a al chatbot responder preguntas basadas en documentos espec√≠ficos
- Ampliar√≠a las capacidades del sistema para casos de uso m√°s avanzados
- Facilitar√≠a la creaci√≥n de asistentes especializados en dominios espec√≠ficos

**Estado**: üìù Pendiente de implementaci√≥n

### Comparaci√≥n de modelos lado a lado
**Propuesta**: Implementar una funcionalidad para comparar respuestas de diferentes modelos a la misma pregunta.

**Beneficios**:
- Facilitar√≠a la evaluaci√≥n de la calidad de diferentes modelos
- Permitir√≠a identificar las fortalezas y debilidades de cada modelo
- Mejorar√≠a la experiencia educativa al mostrar diferentes enfoques

**Estado**: üìù Pendiente de implementaci√≥n

### An√°lisis de rendimiento y uso de tokens
**Propuesta**: A√±adir m√©tricas de rendimiento y conteo de tokens para cada modelo.

**Beneficios**:
- Permitir√≠a optimizar el uso de recursos
- Facilitar√≠a la comparaci√≥n de eficiencia entre modelos
- Ayudar√≠a a estimar costos de uso de la API

**Estado**: üìù Pendiente de implementaci√≥n
